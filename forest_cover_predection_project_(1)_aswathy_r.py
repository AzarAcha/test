# -*- coding: utf-8 -*-
"""forest cover predection project (1) - aswathy r.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbVHuv03oDGeAb17owKJrUZxSn3-IFU5

# PTID-CDS-DEC-24-2228
# PRCP-1005-Forest Cover Prediction

Problem Statement

Task 1:-Prepare a complete data analysis report on the given data.
Task 2:-Create a predictive model which helps to predict seven different cover types in
four different wilderness areas of the Forest with the best accuracy..

Dataset Link:
The actual forest cover type for a given 30 x 30 meter cell was determined from US
Forest Service (USFS) Region 2 Resource Information System data. Independent
variables were then derived from data obtained from the US Geological Survey and
USFS. The data is in raw form (not scaled) and contains binary columns of data for
qualitative independent variables such as wilderness areas and soil type.
This study area includes four wilderness areas located in the Roosevelt National Forest
of northern Colorado. These areas represent forests with minimal human-caused
disturbances, so that existing forest cover types are more a result of ecological
processes rather than forest management practices.
The goal of the Project is to predict seven different cover types in four different
wilderness areas of the Roosevelt National Forest of Northern Colorado with the
best accuracy.

Link : https://d3ilbtxij3aepc.cloudfront.net/projects/CDS-Capstone-Projects/PRCP-1005-
ForestCoverPred.zip

Data Fields
Elevation - Elevation in meters
Aspect - Aspect in degrees azimuth
Slope - Slope in degrees
Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features

Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features
Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway
Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice
Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice
Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice
Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points
Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation
Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation
Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation
The wilderness areas are:
1 - Rawah Wilderness Area
2 - Neota Wilderness Area
3 - Comanche Peak Wilderness Area
4 - Cache la Poudre Wilderness Area
The soil types are:
1 Cathedral family - Rock outcrop complex, extremely stony.
2 Vanet - Ratake families complex, very stony.
3 Haploborolis - Rock outcrop complex, rubbly.
4 Ratake family - Rock outcrop complex, rubbly.
5 Vanet family - Rock outcrop complex complex, rubbly.
6 Vanet - Wetmore families - Rock outcrop complex, stony.
7 Gothic family.
8 Supervisor - Limber families complex.
9 Troutville family, very stony.
10 Bullwark - Catamount families - Rock outcrop complex, rubbly.
11 Bullwark - Catamount families - Rock land complex, rubbly.
12 Legault family - Rock land complex, stony.
13 Catamount family - Rock land - Bullwark family complex, rubbly.
14 Pachic Argiborolis - Aquolis complex.
15 unspecified in the USFS Soil and ELU Survey.

16 Cryaquolis - Cryoborolis complex.
17 Gateview family - Cryaquolis complex.
18 Rogert family, very stony.
19 Typic Cryaquolis - Borohemists complex.
20 Typic Cryaquepts - Typic Cryaquolls complex.
21 Typic Cryaquolls - Leighcan family, till substratum complex.
22 Leighcan family, till substratum, extremely bouldery.
23 Leighcan family, till substratum - Typic Cryaquolls complex.
24 Leighcan family, extremely stony.
25 Leighcan family, warm, extremely stony.
26 Granile - Catamount families complex, very stony.
27 Leighcan family, warm - Rock outcrop complex, extremely stony.
28 Leighcan family - Rock outcrop complex, extremely stony.
29 Como - Legault families complex, extremely stony.
30 Como family - Rock land - Legault family complex, extremely stony.
31 Leighcan - Catamount families complex, extremely stony.
32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.
33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.
34 Cryorthents - Rock land complex, extremely stony.
35 Cryumbrepts - Rock outcrop - Cryaquepts complex.
36 Bross family - Rock land - Cryumbrepts complex, extremely stony.
37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.
38 Leighcan - Moran families - Cryaquolls complex, extremely stony.
39 Moran family - Cryorthents - Leighcan family complex, extremely stony.
40 Moran family - Cryorthents - Rock land complex, extremely stony.
Target variable: Cover_Type (7 categories)

Model Comparison Report
Create a report stating the performance of multiple models on this data and
suggest the best model for production.
Report on Challenges faced
Create a report which should include challenges you faced on data and
what technique used with proper reason.
Note:-All above tasks have to be created on a single jupyter notebook and
share the same for final submission of the project.
"""

pip install xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.tree import plot_tree
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.model_selection import learning_curve

pip install python-docx



# Load the dataset
data = pd.read_csv(r"C:\Users\Admin\Documents\CDS PROJECT\FOREST_COVER_PREDICTION\train.csv")

# Display the first few rows of the dataframe
data.head()

data

# Checking for missing values in the dataset
missing_values = data.isnull().sum()

# Display missing values if any
missing_values[missing_values > 0]

# Checking the data types of all columns
data_types = data.dtypes

# Display the data types
data_types

"""# Preprocessing"""

label_encoder = LabelEncoder()
data['Cover_Type'] = label_encoder.fit_transform(data['Cover_Type'])
data.head()

# Initialize the StandardScaler
scalar = StandardScaler()
num_data = data[["Elevation","Aspect","Slope","Horizontal_Distance_To_Hydrology","Vertical_Distance_To_Hydrology","Horizontal_Distance_To_Roadways","Horizontal_Distance_To_Fire_Points"]]
cat_data = data.drop(["Elevation","Aspect","Slope","Horizontal_Distance_To_Hydrology","Vertical_Distance_To_Hydrology","Horizontal_Distance_To_Roadways","Horizontal_Distance_To_Fire_Points"],axis=1)
scaled_data = scalar.fit_transform(num_data)
num_data1 = pd.DataFrame(scaled_data,columns=num_data.columns)
scaled_data = pd.concat([num_data1,cat_data],axis=1)

scaled_data

"""# Visualization

"""

plt.figure(figsize=(15,60), facecolor='white')
plotnumber =1

for column in num_data:
    ax = plt.subplot(16,3,plotnumber)
    sns.distplot(num_data[column])
    plt.xlabel(column,fontsize=10)
    plotnumber+=1
plt.show()

"""# SKEWNESS

"""

skew=data.skew()
skew_df=pd.DataFrame(skew,index=None,columns=['Skewness'])
plt.figure(figsize=(15,7))
sns.barplot(x=skew_df.index,y='Skewness',data=skew_df)
plt.xticks(rotation=90)

class_dist=data.groupby('Cover_Type').size()
class_label=pd.DataFrame(class_dist,columns=['Size'])
plt.figure(figsize=(8,6))
sns.barplot(x=class_label.index,y='Size',data=class_label)

cont_data=data.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']
data['Cover_Type']=data['Cover_Type'].astype('category') #To convert target class into category

for i, col in enumerate(cont_data.columns):
    plt.figure(i,figsize=(8,4))
    sns.boxplot(x=data['Cover_Type'], y=col, data=data, palette="coolwarm")

"""# Heat_Map"""

plt.figure(figsize=(15,8))
sns.heatmap(cont_data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)

g = sns.PairGrid(cont_data)
g.map(plt.scatter)

"""# FEATURE SELECTION:"""

# Define the features and the target variable
X = scaled_data.drop("Cover_Type", axis=1)
y = scaled_data["Cover_Type"]
model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_)

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(54).plot(kind='bar')
plt.show()

top_9_features = feat_importances.nlargest(54)
selected_columns = data[top_9_features.index]
selected_columns

X=scaled_data.drop("Cover_Type",axis=1)
y=scaled_data["Cover_Type"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Check the shape of the resulting sets
(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""# XGBoost"""

import xgboost as xgb
tree1= xgb.XGBClassifier()
tree1.fit(X_train, y_train)
y_pred0 = tree1.predict(X_test)

from sklearn import metrics
report0=classification_report(y_test,y_pred0)
print(report0)

# Create the confusion matrix
matrix = confusion_matrix(y_test,y_pred0)
# Plot the confusion matrix
plt.matshow(matrix, cmap='Blues')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Random Forest"""

# Initialize Gaussian Naive Bayes since our features are continuous after standardization
nb_model = RandomForestClassifier()

# Train the model
nb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred1 = nb_model.predict(X_test)

report = classification_report(y_test,y_pred1)
print(report)

# Create the confusion matrix
matrix = confusion_matrix(y_test,y_pred1)
# Plot the confusion matrix
plt.matshow(matrix, cmap='Blues')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# KNN"""

# Initialize Gaussian Naive Bayes since our features are continuous after standardization
knn = KNeighborsClassifier()

# Train the model
knn.fit(X_train,y_train)

# Make predictions on the test set
y_pred2 = knn.predict(X_test)

report1=classification_report(y_test,y_pred2)
print(report1)

# Create the confusion matrix
matrix = confusion_matrix(y_test, y_pred2)
# Plot the confusion matrix
plt.matshow(matrix, cmap='Blues')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Logistic Regression"""

# Create the logistic regression model
model = LogisticRegression()

# Fit the model to the data
model.fit(X_train, y_train)

# Make predictions
y_pred3 = model.predict(X_test)

report2=classification_report(y_test,y_pred3)
print(report2)

# Create the confusion matrix
matrix = confusion_matrix(y_test, y_pred3)
# Plot the confusion matrix
plt.matshow(matrix, cmap='Blues')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Decision Tree"""

# Initialize and train the Decision Tree classifier
tree_classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)
tree_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred4 = tree_classifier.predict(X_test)

report=classification_report(y_test,y_pred4)
print(report)

plt.figure(figsize=(65,50))
plot_tree(tree_classifier, filled=True, feature_names=X.columns.tolist(), class_names=['1','2','3','4','5','6','7'], fontsize=30)
plt.title("Decision Tree")
plt.show()

# Create the confusion matrix
matrix = confusion_matrix(y_test, y_pred4)
# Plot the confusion matrix
plt.matshow(matrix, cmap='Blues')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Loss Curve"""

#for decision Tree
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(tree_classifier, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)


plt.figure(figsize=(10, 6))
plt.title("Loss Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
plt.legend(loc="best")

plt.show()

#for Logistic regression



train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)


plt.figure(figsize=(10, 6))
plt.title("Loss Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
plt.legend(loc="best")

plt.show()

#for knn


train_sizes, train_scores, test_scores = learning_curve(knn , X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)


plt.figure(figsize=(10, 6))
plt.title("Loss Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
plt.legend(loc="best")

plt.show()

#for random_forest


train_sizes, train_scores, test_scores = learning_curve(nb_model , X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)


plt.figure(figsize=(10, 6))
plt.title("Loss Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
plt.legend(loc="best")

plt.show()

#for xgboosting


train_sizes, train_scores, test_scores = learning_curve(xgb, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)


plt.figure(figsize=(10, 6))
plt.title("Loss Curve")
plt.xlabel("Training Examples")
plt.ylabel("Score")
plt.grid()

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
plt.legend(loc="best")

plt.show()

"""# Objective
The objective of this project was to develop a predictive model to classify seven forest cover types in four wilderness areas of the Roosevelt National Forest, Colorado, using provided geospatial and ecological data.

## 1. Data Analysis
## Dataset Overview

Total Features: 55 columns (10 numerical features, 44 binary categorical features, and 1 target column)

Target Variable: Cover_Type (seven categories represented by integers 1–7)

Data Type Check: All features were correctly identified as numerical or categorical.

Missing Values: No missing values were detected in the dataset.

Skewness Analysis:
1. Many numerical features displayed high skewness.
2. Applied standardization to mitigate the effect of skewness.
   
## Exploratory Data Analysis
Distribution Plots: Visualized distributions for numerical features to understand the spread and skewness of the data.

Heatmap: Highlighted correlations among continuous variables.

1. Features like Elevation, Aspect, and Slope were moderately correlated with the target variable.
   
Boxplots: Examined feature distributions across different forest cover types.

# 2. Feature Engineering
Feature Scaling: Standardized numerical features using StandardScaler.

Feature Importance Analysis:
1. Employed ExtraTreesClassifier to rank feature importance.
2. Identified the top 9 most influential features for prediction.
3. Visualized feature importance to guide model development.

# 3. Model Development
Implemented multiple machine learning algorithms to evaluate performance. Below are the models and their key results:

## Model 1: XGBoost Classifier
Accuracy: High

Classification Report:

Precision and recall were consistent across classes.

Strengths: Captures complex patterns and relationships.

Confusion Matrix: Demonstrated strong prediction performance across all cover types.

## Model 2: Random Forest Classifier

Accuracy: Very high, comparable to XGBoost.

Strengths: Robust and handles imbalanced data well.

Feature Importance: Provided a clear ranking of influential features.

Confusion Matrix: Showed consistent accuracy for all cover types.

## Model 3: K-Nearest Neighbors (KNN)

Accuracy: Moderate.

Limitations: Computationally expensive for large datasets.

Confusion Matrix: Indicated poor performance for some classes.

## Model 4: Logistic Regression

Accuracy: Low.

Limitations: Could not capture the complexity of the dataset due to linear
decision boundaries.

Confusion Matrix: Poor prediction for most cover types.

## Model 5: Decision Tree Classifier

Accuracy: Moderate.

Visualized Decision Tree: Helped understand feature splits and relationships.

Limitations: Tended to overfit.
"""

# 4. Model Comparison
### Model	                     Accuracy	                 Strengths	                                    Weaknesses
#XGBoost Classifier	              High	           Handles large datasets and complex patterns	     Computationally expensive

#Random Forest Classifier	      High	           Handles imbalanced data, robust	                 Computationally intensive for large trees

#K-Nearest Neighbors (KNN)	    Moderate	       Simple to implement	                             Poor scalability, sensitive to noisy data

#Logistic Regression	              Low	           Simple and interpretable	                         Limited to linear relationships

#Decision Tree Classifier	    Moderate	       Easy to visualize	                             Prone to overfitting

"""# 5. Challenges and Solutions

## 1.High Dimensionality of Data:
Challenge: 44 binary features increased the dimensionality and sparsity.

Solution: Identified top features using ExtraTreesClassifier.
## 2.Imbalanced Distribution of Classes:
Challenge: Cover types were not evenly distributed.

Solution: Random oversampling during training.
## 3.Skewed Numerical Features:
Challenge: Some numerical features were highly skewed.

Solution: Standardized features to ensure uniform scaling.
## 4.Model Overfitting:
Challenge: Decision tree models overfit the training data.

Solution: Limited maximum depth and increased min_samples_split.

# 6. Recommendations
Best Model: Based on accuracy and performance, the Random Forest Classifier is recommended for deployment.
#### Future Enhancements:
Use hyperparameter tuning (e.g., GridSearchCV) to improve model performance further.

Explore ensemble methods combining XGBoost and Random Forest for better results.

Evaluate other dimensionality reduction techniques, like PCA, for high-dimensional data.
# 7. Conclusion
The project successfully demonstrated a workflow for analyzing and predicting forest cover types using machine learning models. The Random Forest Classifier provided the best balance of accuracy, interpretability, and computational efficiency.







"""

